import json
from pickle import DICT
from typing import Dict, Any, List
from zhai_agent.models.chat_state import ChatState
from zhai_agent.rag.rag_manager import RAGManager
from zhai_agent.prompt.prompt_builder import PromptBuilder
from zhai_agent.mirix_memory.memory_agent import MirixMemoryAgent
from zhai_agent.kg.kg_manager import KGManager
from langchain_core.messages import AIMessage, HumanMessage
from zhai_agent.utils.trans_messages_to_string import trans_messages_to_string
from langchain.schema import Document
from zhai_agent.prompt.mirix_memory_prompt import build_mirix_memory_prompt
from zhai_agent.kg.kg_tools import get_kg_tools
from langchain_core.utils.function_calling import convert_to_openai_tool

class WorkflowNodes:
    """
    å·¥ä½œæµèŠ‚ç‚¹ç±»ï¼Œå°è£…å„ç§å·¥ä½œæµèŠ‚ç‚¹çš„é€»è¾‘
    """
    
    def __init__(self, rag_manager: RAGManager, retriever=None, prompt_builder: PromptBuilder = None, mirix_agent: MirixMemoryAgent = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµèŠ‚ç‚¹
        Args:
            rag_manager: RAGç®¡ç†å™¨å®ä¾‹
            retriever: æ–‡æ¡£æ£€ç´¢å™¨
            prompt_builder: æç¤ºæ„å»ºå™¨å®ä¾‹
            mirix_agent: Mirixè®°å¿†ä»£ç†å®ä¾‹
        """
        self.rag_manager = rag_manager
        self.retriever = retriever
        self.context_managers = {}  # å­˜å‚¨ä¸åŒä¼šè¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
        self.prompt_builder = prompt_builder or PromptBuilder()
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„mirix_agentå‚æ•°ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥æ‰åˆ›å»ºé»˜è®¤å®ä¾‹
        self.mirix_agent = mirix_agent or MirixMemoryAgent()
        # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç®¡ç†å™¨
        self.kg_manager = KGManager()
        # ä¼˜åŒ–1: é¢„åŠ è½½å¹¶ç¼“å­˜å·¥å…·
        self.kg_tools = get_kg_tools()
        # ä¼˜åŒ–2: ä½¿ç”¨ LangChain æ ‡å‡†å‡½æ•°è½¬æ¢å·¥å…·æ ¼å¼
        self.openai_tools = [convert_to_openai_tool(t) for t in self.kg_tools]


    def llm_kg_node(self, state: ChatState) -> Dict[str, Any]:
        """
        æ™ºèƒ½èŠå¤©èŠ‚ç‚¹ï¼šLLM å†³ç­– -> (å¯é€‰)è°ƒç”¨ KG å·¥å…· -> ç”Ÿæˆå›å¤
        """
        try:
            # 1. è·å–ç”¨æˆ·è¾“å…¥
            user_message = state.messages[-1].content if state.messages else ""
            
            # 2. è·å–è®°å¿†ä¸Šä¸‹æ–‡
            memory_context = self._get_memory_context(state)
            
            # 3. æ„å»º Prompt 
            system_prompt = self._build_intelligent_system_prompt(memory_context)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            # 4. ç¬¬ä¸€è½® LLM è°ƒç”¨ï¼šå†³ç­–ä¸å·¥å…·è°ƒç”¨
            # ä½¿ç”¨é¢„å¤„ç†å¥½çš„ self.openai_tools
            llm_response = self.rag_manager.llm_client.create_chat_completion(
                messages=messages,
                tools=self.openai_tools,
                tool_choice="auto",
                temperature=0.3
            )
            
            # 5. å¤„ç†å·¥å…·è°ƒç”¨ (ReAct å¾ªç¯çš„ç¬¬ä¸€æ­¥)
            tool_calls = llm_response.get("tool_calls")
            if tool_calls:
                print(f"ğŸ¤– LLM å†³å®šè°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
                
                # å°†åŠ©æ‰‹çš„æ€è€ƒè¿‡ç¨‹åŠ å…¥å†å²
                messages.append({
                    "role": "assistant",
                    "content": llm_response.get("content") or "",  # content å¯èƒ½ä¸º None
                    "tool_calls": tool_calls
                })
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·
                tool_results = self._execute_tool_calls(tool_calls, self.kg_tools)
                
                # å°†å·¥å…·ç»“æœåŠ å…¥å†å²
                for tool_result in tool_results:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_result["call_id"],
                        "content": str(tool_result["result"]) # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    })
                
                # 6. ç¬¬äºŒè½® LLM è°ƒç”¨ï¼šæ ¹æ®å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
                final_response = self.rag_manager.llm_client.create_chat_completion(
                    messages=messages,
                    # ç¬¬äºŒè½®é€šå¸¸ä¸éœ€è¦å†è°ƒç”¨å·¥å…·ï¼Œé™¤éå®ç°å¤šè½®å¾ªç¯
                    tools=self.openai_tools, 
                    tool_choice="none", 
                    temperature=0.3
                )
                ai_response = final_response["content"]
            else:
                # æœªè°ƒç”¨å·¥å…·ï¼Œç›´æ¥ä½¿ç”¨å›å¤
                ai_response = llm_response["content"]
            
            # 7. æ›´æ–°çŠ¶æ€
            # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥å°† AI å›å¤åŠ å…¥ state.messagesï¼Œè€Œä¸ä»…ä»…æ˜¯è¿”å›
            from langchain_core.messages import AIMessage
            state.messages.append(AIMessage(content=ai_response))

        except Exception as e:
            print(f"âŒ æ™ºèƒ½èŠå¤©èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            # é”™è¯¯æ¢å¤æœºåˆ¶
            state.messages.append(AIMessage(content="æŠ±æ­‰ï¼Œç³»ç»Ÿå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚"))
            
        return state.model_dump()
        
    def chat_node(self, state: ChatState) -> Dict[str, Any]:
        """
        çº¯èŠå¤©èŠ‚ç‚¹ï¼Œä¸è°ƒç”¨å·¥å…·ï¼Œä»…åŸºäºå·²æœ‰ä¿¡æ¯è¿›è¡Œå¯¹è¯
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            dict: æ›´æ–°åçš„çŠ¶æ€
        """
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            if not state.messages:
                return state.model_dump()
            last_message = state.messages[-1]
            user_message = last_message.content if hasattr(last_message, 'content') else str(last_message)
            # è°ƒç”¨LLMç”Ÿæˆå›å¤
            ai_response = self._generate_response(user_message, state)
            # åˆ›å»ºAIæ¶ˆæ¯å¹¶æ·»åŠ åˆ°çŠ¶æ€
            ai_message = AIMessage(content=ai_response)
            state.messages.append(ai_message)
            
            print(f"çº¯èŠå¤©å›å¤: {ai_response[:100]}...")
            
        except Exception as e:
            print(f"èŠå¤©èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            # æ·»åŠ é”™è¯¯å›å¤
            error_response = "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·ç¨åå†è¯•ã€‚"
            ai_message = AIMessage(content=error_response)
            state.messages.append(ai_message)
        
        return state.model_dump()
    
    def _get_memory_context(self, state: ChatState) -> str:
        """
        è·å–è®°å¿†ä¸Šä¸‹æ–‡ï¼Œä¼˜å…ˆä½¿ç”¨mirixè®°å¿†
        """
        try:
            # å°è¯•è·å–mirixè®°å¿†
            user_name = state.user_name
            if user_name:
                conversation_buffer = trans_messages_to_string(state.messages[-10:])
                memory_context = self.mirix_agent.extract_memory_for_system_prompt(
                    conversation_buffer, user_name
                )
                if memory_context:
                    return f"ç”¨æˆ·è®°å¿†ä¿¡æ¯ï¼š\n{memory_context}"
        except Exception as e:
            print(f"è·å–mirixè®°å¿†å¤±è´¥: {str(e)}")
        
        # å›é€€åˆ°æ™®é€šè®°å¿†
        try:
            user_id = state.user_id
            if user_id in self.context_managers:
                context_manager = self.context_managers[user_id]
                memories = context_manager.get_context(include_long_memory=True, limit=10)
                if memories:
                    return self._format_conversation_history(memories)
        except Exception as e:
            print(f"è·å–æ™®é€šè®°å¿†å¤±è´¥: {str(e)}")
        
        return "æš‚æ— ç›¸å…³è®°å¿†ä¿¡æ¯"
    
    def _build_intelligent_system_prompt(self, memory_context: str) -> str:
        """
        æ„å»ºå·¥å…·è°ƒç”¨æç¤ºï¼Œä¼ å…¥è®°å¿†ä¸Šä¸‹æ–‡è¿›è¡Œè¾…åŠ©ã€‚
        """
        return self.prompt_builder.get_kg_tools_prompt(memory_context)
    
    
    def _execute_tool_calls(self, tool_calls, available_tools) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå·¥å…·è°ƒç”¨
        """
        results = []
        
        # åˆ›å»ºå·¥å…·æ˜ å°„
        tool_map = {tool.name: tool for tool in available_tools if hasattr(tool, 'name')}
        
        for tool_call in tool_calls:
            try:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                if function_name in tool_map:
                    tool = tool_map[function_name]
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    result = tool.invoke(function_args)
                    results.append({
                        "call_id": tool_call.id,
                        "result": str(result)
                    })
                    print(f"å·¥å…·è°ƒç”¨æˆåŠŸ: {function_name} -> {result}")
                else:
                    results.append({
                        "call_id": tool_call.id,
                        "result": f"é”™è¯¯: æœªæ‰¾åˆ°å·¥å…· {function_name}"
                    })
                    
            except Exception as e:
                error_msg = f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
                print(error_msg)
                results.append({
                    "call_id": tool_call.id,
                    "result": error_msg
                })
        
        return results

    def query_kg_node(self,state:ChatState) -> Dict[str,Any]:
        """KGèŠ‚ç‚¹ï¼Œç”¨äºæŸ¥è¯¢çŸ¥è¯†å›¾è°±"""
        # è·å–ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯
        user_message = state.messages[-1].get('content', '') if state.messages else ""
        # è°ƒç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ¥å£
        kg_response = self.kg_manager.query_kg(user_message)
        # å°†æŸ¥è¯¢ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        state.messages.append(AIMessage(content=kg_response))


        return state.model_dump()

    def rag_node(self, state: ChatState) -> Dict[str, Any]:
        """
        RAGèŠ‚ç‚¹ï¼Œç”¨äºä»çŸ¥è¯†åº“æå–ç›¸å…³æ–‡æ¡£
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            dict: æ›´æ–°åçš„çŠ¶æ€
        """
        # è·å–ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯
        user_message = state.messages[-1].get('content', '') if state.messages else ""
        state.query = user_message
        # æ‰§è¡Œæ–‡æ¡£æ£€ç´¢
        retrieved_docs = self._retrieve_documents(user_message)
        state.retrieved_documents = [
            {"content": doc.page_content, "metadata": doc.metadata}
            for doc in retrieved_docs
        ]
        # å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’
        sorted_docs = self._rerank_documents(retrieved_docs, user_message)
        
        # å°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
        rag_context_str = ""
        for i, doc in enumerate(sorted_docs, 1):
            rag_context_str += f"å‚è€ƒèµ„æ–™{i}ï¼š{doc.page_content}\n"
        
        # ä¿®æ”¹ç‚¹ï¼šä¸å†è°ƒç”¨ self.prompt_builder.build_rag_prompt
        # è€Œæ˜¯æ›´æ–° state
        state.rag_context = rag_context_str

        return state.model_dump()


    def mirix_memory_node(self, state:ChatState) -> Dict[str, Any]:
        """
        MIRIXè®°å¿†èŠ‚ç‚¹ï¼Œç”¨äºä»MIRIXä»£ç†æå–è®°å¿†ä¸Šä¸‹æ–‡
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        """
        # è·å–ç”¨æˆ·å§“å
        user_name = state.user_name
        # ä»MIRIXä»£ç†æå–è®°å¿†ä¸Šä¸‹æ–‡
        memory_context = build_mirix_memory_prompt(
            self.mirix_agent,
            user_name,
            trans_messages_to_string(state.messages)
        )
        state.memory_context = memory_context
        
        return state.model_dump()

    def kg_search_node(self, state: ChatState) -> Dict[str, Any]:
        """
        çŸ¥è¯†å›¾è°±æœç´¢èŠ‚ç‚¹ - å®Œå…¨ç”±LLMå†³ç­–æŸ¥è¯¢ç­–ç•¥
        æµç¨‹ï¼šåˆ†æç”¨æˆ·éœ€æ±‚ â†’ LLMè‡ªä¸»é€‰æ‹©çŸ¥è¯†å›¾è°±å·¥å…·æŸ¥è¯¢ â†’ ç›‘æ§å·¥å…·è°ƒç”¨å¹¶æ•´åˆç»“æœ
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            dict: æ›´æ–°åçš„çŠ¶æ€
        """
        try:
            # è·å–ç”¨æˆ·æ¶ˆæ¯
            if state.messages:
                last_message = state.messages[-1]
                user_message = last_message.content if hasattr(last_message, 'content') else str(last_message)
            else:
                user_message = ""
            
            # æ„å»ºè¯¦ç»†çš„ç³»ç»Ÿæç¤ºï¼Œæ˜ç¡®æŒ‡å¯¼LLMä½¿ç”¨å·¥å…·
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æŸ¥è¯¢ä¸“å®¶ã€‚ä½ å¿…é¡»ä½¿ç”¨æä¾›çš„å·¥å…·æ¥æŸ¥è¯¢çŸ¥è¯†å›¾è°±ï¼Œä¸èƒ½å‡­æƒ³è±¡å›ç­”ã€‚

å¯ç”¨å·¥å…·åŒ…æ‹¬ï¼š
- kg_search_entities(keyword): æœç´¢åŒ…å«å…³é”®è¯çš„å®ä½“ï¼ˆå¦‚äººåã€ç‰©å“ã€æ¦‚å¿µç­‰ï¼‰
- kg_get_entity(entity_name): è·å–å®ä½“çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå±æ€§ã€å…³ç³»ç­‰ï¼‰
- kg_get_graph_stats(): è·å–çŸ¥è¯†å›¾è°±çš„æ•´ä½“ç»Ÿè®¡ä¿¡æ¯

é‡è¦è§„åˆ™ï¼š
1. å¯¹äºç”¨æˆ·å…³äºä¸ªäººå–œå¥½ã€å±æ€§ã€å…³ç³»çš„é—®é¢˜ï¼Œä½ å¿…é¡»å…ˆæœç´¢ç›¸å…³å®ä½“
2. å¦‚æœæ‰¾åˆ°å®ä½“ï¼Œç«‹å³ä½¿ç”¨get_entityè·å–å…¶è¯¦ç»†ä¿¡æ¯
3. ä½¿ç”¨å·¥å…·è·å–çœŸå®æ•°æ®ï¼Œä¸èƒ½å‡­æƒ³è±¡æˆ–å‡è®¾å›ç­”
4. å¦‚æœæœç´¢ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¦å¦‚å®è¯´æ˜"åœ¨çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
5. è®°ä½ç”¨æˆ·çš„åå­—å’Œä¸ªäººä¿¡æ¯å¾ˆé‡è¦ï¼Œæ¯æ¬¡å¯¹è¯éƒ½è¦æ£€æŸ¥çŸ¥è¯†å›¾è°±

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
1. æå–ç”¨æˆ·é—®é¢˜ä¸­çš„å…³é”®å®ä½“åç§°ï¼ˆå¦‚äººå"ç¹èŠ±"ï¼‰
2. é¦–å…ˆä½¿ç”¨kg_search_entitiesæœç´¢è¯¥å®ä½“ï¼ˆä¸æŒ‡å®šå®ä½“ç±»å‹ï¼‰
3. å¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯ä»¥å°è¯•æŒ‡å®šå¸¸è§ç±»å‹å¦‚'person'å†æ¬¡æœç´¢
4. å¦‚æœæ‰¾åˆ°åŒ¹é…å®ä½“ï¼Œä½¿ç”¨kg_get_entityè·å–å…¶å®Œæ•´ä¿¡æ¯
5. åŸºäºå·¥å…·è¿”å›çš„çœŸå®æ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜

è®°å¿†ä¿¡æ¯ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
{self._get_memory_context(state)}

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæ™ºèƒ½åœ°é€‰æ‹©æŸ¥è¯¢å·¥å…·å¹¶æ‰§è¡ŒæŸ¥è¯¢ã€‚è®°ä½ï¼šå¿…é¡»ä½¿ç”¨å·¥å…·è·å–çœŸå®æ•°æ®ï¼"""
            
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            # è·å–çŸ¥è¯†å›¾è°±å·¥å…·ï¼Œä½†åªä¿ç•™æŸ¥è¯¢å·¥å…·
            from zhai_agent.kg.kg_tools import get_kg_tools
            all_tools = get_kg_tools()
            
            print(f"è·å–åˆ° {len(all_tools)} ä¸ªå·¥å…·")
            
            # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªå·¥å…·
            for i, tool in enumerate(all_tools):
                tool_name = getattr(tool, 'name', f'unknown_{i}')
                tool_type = type(tool)
                print(f"å·¥å…· {i}: name='{tool_name}', type={tool_type}")
            
            # è°ƒè¯•ï¼šç›´æ¥æŸ¥çœ‹çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“ç»Ÿè®¡
            try:
                from zhai_agent.kg.kg_tools import get_graph_stats
                stats = get_graph_stats({})
                print(f"\nğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡: {str(stats)[:200]}...")
            except Exception as e:
                print(f"\nè·å–å›¾è°±ç»Ÿè®¡å¤±è´¥: {e}")
            
            # è¿‡æ»¤å‡ºä»…æŸ¥è¯¢å·¥å…·å¹¶è®°å½•
            query_tools = []
            for tool in all_tools:
                if hasattr(tool, 'name') and tool.name in ['kg_search_entities', 'kg_get_entity', 'kg_get_graph_stats']:
                    query_tools.append(tool)

            print(f"å¯ç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢å·¥å…·: {[tool.name for tool in query_tools]}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·ï¼Œå°è¯•ä½¿ç”¨æ‰€æœ‰å·¥å…·
            if not query_tools:
                print("âš ï¸  æœªæ‰¾åˆ°æŒ‡å®šçš„æŸ¥è¯¢å·¥å…·ï¼Œå°è¯•ä½¿ç”¨æ‰€æœ‰å·¥å…·...")
                query_tools = all_tools
                print(f"ä½¿ç”¨æ‰€æœ‰å·¥å…·: {[getattr(tool, 'name', 'unknown') for tool in query_tools]}")
                
                # å¦‚æœä»ç„¶æ²¡æœ‰å·¥å…·ï¼Œåˆ›å»ºæ¨¡æ‹Ÿå·¥å…·
                if not query_tools:
                    print("âŒ æ²¡æœ‰ä»»ä½•å·¥å…·å¯ç”¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿå·¥å…·...")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡æ‹Ÿå·¥å…·æˆ–é”™è¯¯å¤„ç†
            
            # å°†å·¥å…·è½¬æ¢ä¸ºOpenAIæ ¼å¼
            tools = self._convert_tools_to_openai_format(query_tools)
            
            # è°ƒç”¨æ”¯æŒå·¥å…·çš„LLMè¿›è¡ŒæŸ¥è¯¢ - å¼ºåˆ¶ä½¿ç”¨å·¥å…·
            llm_response = self.rag_manager.llm_client.create_chat_completion(
                messages=messages,
                tools=tools,
                tool_choice="required",  # å¼ºåˆ¶LLMå¿…é¡»ä½¿ç”¨å·¥å…·
                temperature=0.1  # é™ä½æ¸©åº¦ä»¥æé«˜ç¡®å®šæ€§
            )
            
            # è¯¦ç»†ç›‘æ§å·¥å…·è°ƒç”¨æƒ…å†µ
            tool_usage_info = []
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if llm_response.get("tool_calls"):
                print(f"\nâœ“ LLMå†³å®šè°ƒç”¨ {len(llm_response['tool_calls'])} ä¸ªçŸ¥è¯†å›¾è°±æŸ¥è¯¢å·¥å…·")
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = self._execute_tool_calls(llm_response["tool_calls"], query_tools)
                
                # æ”¶é›†æ‰€æœ‰æŸ¥è¯¢ç»“æœå¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
                for i, tool_result in enumerate(tool_results):
                    tool_call = llm_response["tool_calls"][i]
                    # æ­£ç¡®å¤„ç†ChatCompletionMessageFunctionToolCallå¯¹è±¡
                    if hasattr(tool_call, 'function'):
                        tool_name = getattr(tool_call.function, 'name', 'unknown')
                        # è·å–å·¥å…·è°ƒç”¨çš„å‚æ•°
                        if hasattr(tool_call.function, 'arguments'):
                            try:
                                import json
                                args = json.loads(tool_call.function.arguments)
                                print(f"  - å·¥å…· {i+1}: {tool_name} å‚æ•°: {args}")
                                
                                # å¦‚æœæœç´¢å®ä½“æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•ä¸æŒ‡å®šç±»å‹çš„æœç´¢
                                if tool_name == 'kg_search_entities' and 'æœªæ‰¾åˆ°åŒ…å«' in str(tool_result['result']):
                                    print(f"  - æœç´¢æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•ä¸æŒ‡å®šå®ä½“ç±»å‹çš„æœç´¢...")
                                    # é‡æ–°æœç´¢ï¼Œä¸æŒ‡å®šentity_type
                                    from zhai_agent.kg.kg_tools import search_entities
                                    retry_result = search_entities(args.get('keyword', ''))
                                    print(f"  - é‡æ–°æœç´¢ç»“æœ: {str(retry_result)[:200]}...")
                                    if 'æœªæ‰¾åˆ°åŒ…å«' not in retry_result:
                                        tool_result['result'] = retry_result
                                        print(f"  - âœ… é‡æ–°æœç´¢æˆåŠŸï¼")
                                    else:
                                        # å°è¯•ç›´æ¥æŸ¥è¯¢æ‰€æœ‰å®ä½“
                                        print(f"  - å°è¯•ç›´æ¥æŸ¥è¯¢æ‰€æœ‰å®ä½“...")
                                        try:
                                            from zhai_agent.kg.kg_storage import KGStorage
                                            storage = KGStorage()
                                            all_entities = storage.search_entities('ç¹èŠ±')
                                            print(f"  - ç›´æ¥æŸ¥è¯¢ç»“æœ: æ‰¾åˆ° {len(all_entities)} ä¸ªå®ä½“")
                                            for entity in all_entities[:5]:
                                                print(f"    - å®ä½“: {entity.get('name', 'unknown')} (ç±»å‹: {entity.get('type', 'unknown')})")
                                        except Exception as debug_e:
                                            print(f"  - ç›´æ¥æŸ¥è¯¢å¤±è´¥: {debug_e}")
                                    
                            except:
                                pass
                    else:
                        tool_name = 'unknown'
                    tool_usage_info.append(f"å·¥å…·: {tool_name}, ç»“æœ: {str(tool_result['result'])[:200]}")
                    print(f"    ç»“æœ: {str(tool_result['result'])[:200]}...")
            else:
                print(f"\nâœ— LLMæœªè°ƒç”¨ä»»ä½•å·¥å…·ï¼")
                print(f"  è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š")
                print(f"  1. LLMè®¤ä¸ºä¸éœ€è¦æŸ¥è¯¢çŸ¥è¯†å›¾è°±")
                print(f"  2. ç³»ç»Ÿæç¤ºä¸å¤Ÿæ˜ç¡®")
                print(f"  3. å·¥å…·é€‰æ‹©é€»è¾‘é—®é¢˜")
                tool_usage_info.append("LLMæœªè°ƒç”¨ä»»ä½•å·¥å…·")
            
            # æ•´åˆæŸ¥è¯¢ç»“æœ
            if tool_usage_info:
                kg_context_str = f"çŸ¥è¯†å›¾è°±æŸ¥è¯¢æƒ…å†µ:\n" + "\n".join(tool_usage_info)
            else:
                kg_context_str = "çŸ¥è¯†å›¾è°±ä¸­æ— ç›¸å…³ä¿¡æ¯"
            
            # å°†æŸ¥è¯¢ç»“æœæ·»åŠ åˆ°æç¤ºè¯æ„å»ºå™¨ä¸­
            state.kg_context = kg_context_str
            
        except Exception as e:
            error_message = f"çŸ¥è¯†å›¾è°±æœç´¢èŠ‚ç‚¹å‡ºé”™: {str(e)}"
            print(error_message)
            import traceback
            traceback.print_exc()
            
            # æä¾›æ›´æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯
            error_context = f"çŸ¥è¯†å›¾è°±æŸ¥è¯¢å‡ºé”™: {str(e)}\nè¿™å¯èƒ½æ˜¯å› ä¸ºï¼š\n1. çŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯\n2. å®ä½“åç§°æ‹¼å†™ä¸åŒ\n3. è¯¥å®ä½“å°šæœªè¢«è®°å½•åˆ°çŸ¥è¯†å›¾è°±ä¸­\nå»ºè®®ï¼šå¯ä»¥è¯¢é—®ç”¨æˆ·çš„å…·ä½“å–œå¥½ï¼Œç„¶åè®°å½•ä¸‹æ¥ã€‚"
            
            # å°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°æç¤ºè¯ä¸­
            self.prompt_builder.build_kg_prompt(error_context)
        
        return state.model_dump()

    def normal_memory_node(self, state: ChatState) -> Dict[str, Any]:
        """
        æ™®é€šè®°å¿†èŠ‚ç‚¹ï¼Œç”¨äºè·å–åŸºäºç”¨æˆ·IDçš„è®°å¿†ä¸Šä¸‹æ–‡
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        """
        # ä½¿ç”¨user_idä½œä¸ºä¼šè¯æ ‡è¯†ç¬¦
        user_id = state.user_id
        
        # å¦‚æœç”¨æˆ·è¿˜æ²¡æœ‰ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œåˆ›å»ºä¸€ä¸ª
        if user_id not in self.context_managers:
            self.context_managers[user_id] = get_mcp_context(
                user_id=user_id,
                redis_password='huafan123',
                pg_password='huafan123'
            )
        
        # è·å–å½“å‰ç”¨æˆ·çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager = self.context_managers[user_id]
        
        # ä»ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–è®°å¿†ï¼ˆåŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼‰
        previous_messages = context_manager.get_context(include_long_memory=True, limit=10)
        print(f"ä»MCPContextManageråŠ è½½çš„æ¶ˆæ¯æ•°é‡: {len(previous_messages)}")
        
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        conversation_history = self._format_conversation_history(previous_messages)

        self.prompt_builder.build_memory_prompt(conversation_history)
        
        return state.model_dump()


    def _retrieve_documents(self, user_message: str) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
        Returns:
            List[Document]: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        retrieved_docs = []
        # å¦‚æœæœ‰æ£€ç´¢å™¨ï¼Œæ‰§è¡Œæ–‡æ¡£æ£€ç´¢
        if self.retriever:
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.rag_manager.retrieve_documents(self.retriever, user_message)
            print(f"\nå·²æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        else:
            print("\næœªä½¿ç”¨RAGå¢å¼ºï¼Œæ— æ–‡æ¡£æ£€ç´¢æ­¥éª¤")
        return retrieved_docs
    

    def _rerank_documents(self, retrieved_docs: List[Document], user_message: str) -> List[Document]:
        """
        å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’
        Args:
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            user_message: ç”¨æˆ·æ¶ˆæ¯
        Returns:
            List[Document]: é‡æ’åçš„æ–‡æ¡£åˆ—è¡¨
        """
        return self.rag_manager.reRank(retrieved_docs, user_message)
    

    def _format_conversation_history(self, previous_messages: List[HumanMessage | AIMessage]) -> str:
        """
        æ ¼å¼åŒ–å¯¹è¯å†å²
        Args:
            previous_messages: å†å²æ¶ˆæ¯åˆ—è¡¨
        Returns:
            str: æ ¼å¼åŒ–çš„å¯¹è¯å†å²
        """
        conversation_history = ""
        if previous_messages:
            for msg in previous_messages:
                if msg.type == 'human':
                    conversation_history += f"ç”¨æˆ·: {msg.content}\n"
                elif msg.type == 'ai':
                    conversation_history += f"åŠ©æ‰‹: {msg.content}\n"
            # ç§»é™¤æœ€åä¸€ä¸ªæ¢è¡Œç¬¦
            if conversation_history:
                conversation_history = conversation_history.rstrip('\n')
            print("å·²æ·»åŠ å¯¹è¯å†å²åˆ°æç¤ºä¸­")
        else:
            print("æ— å¯¹è¯å†å²")
        return conversation_history


    def _generate_response(self, query: str, state: ChatState) -> str:
        """
        ç”ŸæˆAIå“åº”
        """
        # æ­¤æ—¶æ˜¯ä» state ä¸­è¯»å–æ•°æ®ï¼Œè€Œä¸æ˜¯ä» prompt_builder çš„å†…éƒ¨å˜é‡è¯»å–
        final_prompt = self.prompt_builder.build_final_prompt(
            query=query,
            memory_context=state.memory_context,
            rag_context=state.rag_context,
            kg_context=state.kg_context
        )
        
        print(f"ç”Ÿæˆçš„æœ€ç»ˆæç¤º:\n{final_prompt}")
        return self.rag_manager.call_llm(final_prompt)
    
    def store_mirix_memory_node(self, state: ChatState) -> Dict[str, Any]:
        """
        MIRIXè®°å¿†ä¿å­˜æ›´æ–°èŠ‚ç‚¹ï¼ŒåŸºäºç”¨æˆ·å§“åä¿å­˜è®°å¿†
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        """
        # ä½¿ç”¨user_nameæ ‡è¯†ç”¨æˆ·
        user_name = state.user_name
        self.mirix_agent.add_memory(trans_messages_to_string(state.messages), user_name=user_name)
        return state.model_dump()
      

    def store_memory_node(self, state: ChatState) -> Dict[str, Any]:
        """
        è®°å¿†å­˜å‚¨èŠ‚ç‚¹ï¼Œç”¨äºå°†å¯¹è¯å†…å®¹ä¿å­˜åˆ°çŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼ŒåŸºäºç”¨æˆ·IDä¿å­˜
        Args:
            state: èŠå¤©çŠ¶æ€
        Returns:
            dict: æ›´æ–°åçš„çŠ¶æ€
        """
        try:
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ¶ˆæ¯è¿›è¡Œå­˜å‚¨
            if len(state.messages) >= 2:
                # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å’ŒAIå›å¤
                user_msg = state.messages[-2]
                ai_msg = state.messages[-1]
                # ä½¿ç”¨user_idä½œä¸ºä¼šè¯æ ‡è¯†
                user_id = state.user_id
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥ç”¨æˆ·çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                if user_id in self.context_managers:
                    context_manager = self.context_managers[user_id]
                    
                    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å­˜å‚¨æ¶ˆæ¯
                    self._store_messages(context_manager, user_msg, ai_msg)
                    
                    # è·å–å¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                    stats = context_manager.get_stats()
                    print(f"ç”¨æˆ·{user_id}ç»Ÿè®¡: çŸ­æœŸè®°å¿†{stats['short_memory_count']}æ¡, é•¿æœŸè®°å¿†{stats['long_memory_count']}æ¡")
        except Exception as e:
            print(f"ä¿å­˜å¯¹è¯åˆ°è®°å¿†æ—¶å‡ºé”™: {str(e)}")
        return state.model_dump()

    
    def _store_messages(self, context_manager, user_msg, ai_msg):
        """
        å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯å’ŒAIå›å¤
        Args:
            context_manager: ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹
            user_msg: ç”¨æˆ·æ¶ˆæ¯
            ai_msg: AIå›å¤
        """
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        context_manager.add_user_message(
            content=getattr(user_msg, 'content', ''),
            importance_score=0.5  # ä¸­ç­‰é‡è¦æ€§
        )
        
        # ä¿å­˜AIå›å¤
        context_manager.add_ai_message(
            content=getattr(ai_msg, 'content', ''),
            importance_score=0.5  # ä¸­ç­‰é‡è¦æ€§
        )
        
        user_id = context_manager.user_id
        print(f"å¯¹è¯å†…å®¹å·²é€šè¿‡MCPContextManagerä¿å­˜åˆ°ç”¨æˆ·: {user_id}")
    

  